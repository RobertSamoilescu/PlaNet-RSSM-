{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch.optim as optim\n",
    "from planet.models.determinstic_state import DeterministicStateModel\n",
    "from planet.models.stochastic_state import StochasticStateModel\n",
    "from planet.models.reward import RewardModel\n",
    "from planet.models.encoder import ImageEncoderModel\n",
    "from planet.models.observation import ImageObservationModel\n",
    "from planet.utils.seed import set_seed\n",
    "from planet.utils.envs import make_env\n",
    "from planet.trainer import load_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "# set seed for reproducibility\n",
    "set_seed(13)\n",
    "\n",
    "domain_name = \"walker\"\n",
    "task_name = \"walk\"\n",
    "action_repeat = 2\n",
    "\n",
    "# domain_name = \"cartpole\"\n",
    "# task_name = \"balance\"\n",
    "# action_repeat = 8\n",
    "\n",
    "# %%\n",
    "free_nats = 3.0\n",
    "env_config = {\n",
    "    \"env_type\": \"dm_control\",\n",
    "    \"domain_name\": domain_name, \n",
    "    \"task_name\": task_name, \n",
    "    \"render_kwargs\": {'width': 64, 'height': 64, 'camera_id': 0},\n",
    "    \"skip\": action_repeat,\n",
    "}\n",
    "\n",
    "env = make_env(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "action_size = action.shape[0]\n",
    "\n",
    "state_size = 30\n",
    "hidden_state_size = 200\n",
    "observation_size = 1024\n",
    "hidden_layer_size = 200\n",
    "\n",
    "det_state_model = DeterministicStateModel(\n",
    "    hidden_state_size=hidden_state_size,\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    hidden_layer_size=hidden_layer_size\n",
    ").cuda()\n",
    "\n",
    "stoch_state_model = StochasticStateModel(\n",
    "    hidden_state_size=hidden_state_size,\n",
    "    state_size=state_size,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    ").cuda()\n",
    "\n",
    "obs_model = ImageObservationModel(\n",
    "    hidden_state_size=hidden_state_size,\n",
    "    state_size=state_size,\n",
    "    observation_size=observation_size,\n",
    ").cuda()\n",
    "\n",
    "reward_obs_model = RewardModel(\n",
    "    hidden_state_size=hidden_state_size,\n",
    "    state_size=state_size,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    ").cuda()\n",
    "\n",
    "enc_model = ImageEncoderModel(\n",
    "    hidden_state_size=hidden_state_size,\n",
    "    observation_size=observation_size,\n",
    "    state_size=state_size,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    ").cuda()\n",
    "\n",
    "models = {\n",
    "    \"det_state_model\": det_state_model,\n",
    "    \"stoch_state_model\": stoch_state_model,\n",
    "    \"obs_model\": obs_model,\n",
    "    \"reward_obs_model\": reward_obs_model,\n",
    "    \"enc_model\": enc_model,\n",
    "}\n",
    "\n",
    "lr = 6e-4\n",
    "all_params = list(det_state_model.parameters()) + list(stoch_state_model.parameters()) + list(obs_model.parameters()) + list(reward_obs_model.parameters()) + list(enc_model.parameters())\n",
    "optimizers = {\n",
    "    \"all_params\": optim.Adam(\n",
    "        all_params,\n",
    "        lr=lr, \n",
    "    ),\n",
    "}\n",
    "\n",
    "load_models(models, optimizers, f\"checkpoints-%s-%.2f/best_model.pth\" % (domain_name, free_nats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"env_config\": env_config,\n",
    "    \"train_config\": {\n",
    "        \"S\": 5,\n",
    "        \"train_steps\": 2_000,\n",
    "        \"C\": 100,\n",
    "        \"B\": 50,\n",
    "        \"L\": 50,\n",
    "        \"H\": 15,\n",
    "        \"I\": 10,\n",
    "        \"J\": 1000,\n",
    "        \"K\": 100,\n",
    "        \"log_interval\": 1,\n",
    "        \"action_noise\": 0.1,\n",
    "        \"free_nats\": free_nats,\n",
    "        \"checkpoint_dir\": \"checkpoints-walker-%.2f\" % free_nats,\n",
    "        \"max_episode_length\": 1000,\n",
    "        \"action_repeat\": action_repeat,\n",
    "        \"all_params\": all_params\n",
    "    },\n",
    "    \"state_config\": {\n",
    "        \"hidden_state_size\": hidden_state_size,\n",
    "        \"state_size\": state_size,\n",
    "        \"action_size\": action_size,\n",
    "    },\n",
    "    \"eval_config\": {\n",
    "        \"eval_interval\": 25,\n",
    "        \"num_eval_episodes\": 5,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Optional\n",
    "from planet.trainer import _set_models_eval\n",
    "from planet.planning.planner import latent_planning\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_episode(\n",
    "    env: Any, action_noise: Optional[float] = None\n",
    "):\n",
    "    _set_models_eval(models)\n",
    "\n",
    "    # reset environment\n",
    "    sequence, episode_reward = [], 0\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # initialize hidden state and state belief\n",
    "    hidden_state = torch.zeros(\n",
    "        1, config[\"state_config\"][\"hidden_state_size\"]\n",
    "    ).cuda()\n",
    "\n",
    "    max_episode_length = config[\"train_config\"][\"max_episode_length\"]\n",
    "    action_repeat = config[\"train_config\"][\"action_repeat\"]\n",
    "    T = max_episode_length // action_repeat\n",
    "\n",
    "    for _ in tqdm(range(T)):\n",
    "        sequence.append(obs)\n",
    "        observation = torch.from_numpy(obs).float().unsqueeze(0).cuda()\n",
    "        posterior_dist = models[\"enc_model\"](\n",
    "            hidden_state=hidden_state,\n",
    "            observation=observation,\n",
    "        )\n",
    "        action = latent_planning(\n",
    "            H=config[\"train_config\"][\"H\"],\n",
    "            I=config[\"train_config\"][\"I\"],\n",
    "            J=config[\"train_config\"][\"J\"],\n",
    "            K=config[\"train_config\"][\"K\"],\n",
    "            hidden_state=hidden_state,\n",
    "            current_state_belief=posterior_dist,\n",
    "            deterministic_state_model=models[\"det_state_model\"],\n",
    "            stochastic_state_model=models[\"stoch_state_model\"],\n",
    "            reward_model=models[\"reward_obs_model\"],\n",
    "            action_size=config[\"state_config\"][\"action_size\"],\n",
    "        )\n",
    "\n",
    "        # add exploration noise\n",
    "        if action_noise is not None:\n",
    "            action += torch.randn_like(action) * action_noise\n",
    "\n",
    "        # take action in the environment\n",
    "        action_cpu = action.cpu()\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(\n",
    "            action_cpu.numpy()\n",
    "        )\n",
    "\n",
    "        # update episode reward and add\n",
    "        # step to the sequence\n",
    "        episode_reward += reward\n",
    "        done = 1 if terminated or truncated else 0\n",
    "        if done == 1:\n",
    "            break\n",
    "\n",
    "        # update observation\n",
    "        obs = next_obs\n",
    "\n",
    "        # update hidden state\n",
    "        hidden_state = models[\"det_state_model\"](\n",
    "            hidden_state=hidden_state,\n",
    "            state=posterior_dist.sample(),\n",
    "            action=action.unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    return sequence, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "def tensors_to_gif(\n",
    "    tensor_list: List[np.ndarray], filename: str, duration=2.0, value_range: Tuple[int, int] = (-1, 1)\n",
    ") -> None:\n",
    "    \"\"\"Save a list of tensors as a GIF file.\n",
    "\n",
    "    :param tensor_list: List of tensors\n",
    "    :param filename: Name of the GIF file\n",
    "    :param duration: Duration of each frame in seconds\n",
    "    \"\"\"\n",
    "    images = []\n",
    "\n",
    "    for image in tensor_list:\n",
    "        image = image.transpose(1, 2, 0) + 0.5\n",
    "        # resize image\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "        # Ensure pixel values are in the range [0, 255] and of integer type\n",
    "        image = (image * 255).astype(\"uint8\")\n",
    "        images.append(image)\n",
    "\n",
    "    # Write images to GIF using imageio\n",
    "    imageio.mimsave(filename, images, format=\"GIF\", duration=duration, loop=0)  # type: ignore[call-overload]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "sequences = []\n",
    "\n",
    "for i in range(5):\n",
    "    sequence, reward = collect_episode(env, action_noise=None)\n",
    "    sequences.append(sequence)\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_to_gif(sequences[1], f\"{domain_name}.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
